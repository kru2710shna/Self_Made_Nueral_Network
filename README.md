# Self_Made_Neural_Network

This project demonstrates the implementation of a simple neural network from scratch in Python. It covers fundamental components like dense layers, activation functions, and normalization techniques. The code is modular and easy to understand, making it a great resource for learning and experimenting with basic neural network concepts.

## Features
- Custom implementation of dense (fully connected) layers.
- ReLU (Rectified Linear Unit) activation function.
- Normalization of layer outputs (similar to softmax).
- Support for experimenting with datasets like `spiral_data` (commented out in the code).

---

## How to Use the Code

1. Clone the repository:
   ```bash
   git clone https://github.com/your-username/Self_Made_Neural_Network.git
   cd Self_Made_Neural_Network
   ```
## Install the required libraries:
```bash
pip install numpy matplotlib nnfs
```

## Run the script
```bash
python script_name.py
```

## Customization

Modify the number of neurons and inputs in Layer_Dense to observe the impact on network performance.

Replace the dataset or implement new activation functions to test different scenarios.

## Contributions

Feel free to contribute to this project by creating pull requests or raising issues
